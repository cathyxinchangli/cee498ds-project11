## Discussion
### Performance of the linear regression model
Is an obtained RMSLE of 4,5 the limit for linear regression? Most likely not. In the model and data preparation, several things can be made better. First, the datasets could be optimized even better. Site 13 was removed because the values were suspiciously high, as seen in the EDA. In this case, an even more in-depth "cleaning" could be made to locate the exact building (or several building) that is responsible for the data anomaly. This deeper cleaning was made for the other models, but not for the linear regression model. Furthermore, if categorical values could be implemented not only in the training, but also the prediction, perhaps a better score could be obtained. However, it must be realized that no matter how many parameters are added to a linear model, it will still only predict new values linearly. If many or strong non-linear relationships exist between the target variable and prediction features, a linear model will never be able to perform nearly as good as e.g. neural networks. This suggests that other AI-models such as Neural Networks must be used.

### Performance of the RNN-LSTM model
The simple LSTM model was fairly effective, largely outperforming the baseline linear regression model but slightly underperforming than the lightgbm models. 
Tuning attempted improved the performance marginally but steadily. Other tuning opportunities we hope to explore if we had more time include: 
excluding some correlated features; 
increasing model complexity by adding one or more layers; 
other ways of handling missing data. 

It is hard to say for certain whether with more tuning, this three-layer RNN-LSTM would outperform our LightGBM models, but our best guess is no. 
The trade-off between number of samples and number of timestamps means we are forced to leave behind part of the information from the raw data
in training. This can potentially be viewed as a shortcoming for RNN-LSTM (or rather our way of handling it). In hindsight, `building_id` proved to be an important predictor, but treating each building-meter pair as a sample forbade us to use `building_id` as a feature. This also potentially limited the performance of our RNN-LSTM model.

### Tree-based Model and Neural Networks
The dataset in this project is well-structured tabulated data, which is natural for tree-based algorithms. Neural networks usually outperform tree-based algorithms with unstructured data such as images, text, and audio data. And in general, tree-based algorithms are faster to train compared to neural networks.

Moreover, there are several important categorical features in the dataset, such as building_id, site_id, primary_use, month_of_a_year, etc. Among those features, building_id has 1449 classes, which is impossible to one-hot encode due to the RAM limit. Since LGBM deals with categorical variables using fisher's method, none of the encoding procedures are required. 

Lastly, the dataset of the project is relatively large. With advantages such as the ability to reduce memory usage, LGBM is a reasonable choice.

It's worth noticing that LGBM is more sensitive to overfitting compared to other tree-based algorithms such as random forest. 

### Challenges
The most significant challenge has been combating the limited memory resources. 
A significant amount of time was spent on optimizing the memory usage; 
which is also helpful as that potentially has also improved the speed of training/predicting, 
and in the long run building foundations for dealing with larger data and more complex problems and models in the future. 

Another challenge was data preprocessing. For the LightGBM model, simple time features such as month of year, hour of day can be obtained pretty easily, but to further improve the model performance, lag features are recommended. However, we were not able to implement it in our model. As for RNN-LSTM, massaging the training data into a the same shape was challenging, and our decisions of truncating the data and treating each building-meter pair as a sample have probably limited its predicting power. 



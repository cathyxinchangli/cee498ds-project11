## Discussion
### The performance of the RNN-LSTM model
The simple LSTM model was fairly effective, largely outperforming the baseline linear regression model but slightly underperforming than the lightgbm models. 
Tuning attempted improved the performance marginally but steadily. Other tuning opportunities we hope to explore if we had more time include: 
excluding some correlated features; 
increasing model complexity by adding one or more layers; 
other ways of handling missing data. 

It is hard to say for certain whether with more tuning, this three-layer RNN-LSTM would outperform our LightGBM models, but our best guess is no. 
The trade-off between number of samples and number of timestamps means we are forced to leave behind part of the information from the raw data
in training. This can potentially be viewed as a shortcoming for RNN-LSTM (or rather our way of handling it).


### Tree-based Model and Neural Networks
ASSIGNED TO: Minyu & Zhiyi


### Challenges
The most significant challenge has been combating the limited memory resources. 
A significant amount of time was spent on optimizing the memory usage; 
which is also helpful as that potentially has also improved the speed of training/predicting, 
and in the long run building foundations for dealing with larger data and more complex problems and models in the future. 

Another challenge was data preprocessing. 


